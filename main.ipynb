{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "679fee61",
   "metadata": {},
   "source": [
    "# ROI‑Supervised Deep Learning for HER2 Breast Cancer Classification\n",
    "\n",
    "## Abstract\n",
    "\n",
    "\n",
    "## Methods overview\n",
    "- Phase 1 — ROI‑supervised classification: ResNet‑50 backbone; patch extraction strictly within annotated ROIs; wandb metrics and artifacts.\n",
    "- Phase 2 — MIL fine‑tuning: attention‑based MIL initialized from Phase 1; bag‑level training; attention/Grad‑CAM visualizations.\n",
    "- Phase 3 — ROI‑derived segmentation: U‑Net with ResNet backbone; ROI polygons converted to masks; Dice/IoU and pixel accuracy.\n",
    "\n",
    "## Reproducible setup and execution\n",
    "- Prerequisites: Python 3.8+, PyTorch (CUDA‑enabled if available), GPU ≥8 GB VRAM recommended, 16–32 GB RAM, SVS slides with XML annotations.\n",
    "- Execution protocol: restart kernel, run cells sequentially, verify data paths and CUDA availability in config, wandb logging is enabled by default.\n",
    "- Expected outputs: model checkpoints and configs, wandb runs with metrics/artifacts, Grad‑CAM overlays, summary tables/figures.\n",
    "\n",
    "## Experiment tracking (wandb)\n",
    "- Metrics: loss (train/val), LR, early‑stopping; accuracy, precision/recall/F1, ROC‑AUC; confusion matrix; segmentation Dice/IoU.\n",
    "- Artifacts and lineage: versioned checkpoints (by phase/fold), exported configs, selected overlays; dataset fingerprints and code version when available.\n",
    "- Multi‑phase consistency: links Phase 1→2→3 to analyze feature transfer and ROI utilization; includes attention weights and Grad‑CAM when generated.\n",
    "\n",
    "## Notes\n",
    "- Data privacy/IRB compliance is required for clinical datasets.\n",
    "- Seeds are fixed where supported; minor nondeterminism can remain on CUDA/cuDNN.\n",
    "- Report hardware specifications and preserve artifacts to facilitate replication."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bdfd4d90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available: NVIDIA GeForce RTX 4060 Laptop GPU\n",
      "Memory: 8.6 GB\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Environment setup and dependencies\n",
    "==================================\n",
    "\n",
    "Configures the runtime environment and imports required libraries for the HER2 classification pipeline.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "\n",
    "# Environment configuration for stable, reproducible runs\n",
    "os.environ['KMP_DUPLICATE_LIB_OK'] = 'TRUE'  # OpenMP compatibility\n",
    "os.environ['OMP_NUM_THREADS'] = '1'          # Threading optimization\n",
    "warnings.filterwarnings('ignore')             # Suppress non-critical warnings\n",
    "\n",
    "# Python 3.13 compatibility shim\n",
    "import collections\n",
    "import collections.abc\n",
    "if not hasattr(collections, 'Callable'):\n",
    "    collections.Callable = collections.abc.Callable\n",
    "\n",
    "# Core scientific computing libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "from IPython.display import clear_output, display\n",
    "\n",
    "# Jupyter notebook configuration\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "# Configure matplotlib for publication-quality figures\n",
    "plt.rcParams['figure.dpi'] = 150\n",
    "plt.rcParams['savefig.dpi'] = 300\n",
    "plt.rcParams['font.size'] = 12\n",
    "plt.rcParams['axes.titlesize'] = 14\n",
    "plt.rcParams['axes.labelsize'] = 12\n",
    "plt.rcParams['legend.fontsize'] = 10\n",
    "\n",
    "# Add project root to Python path\n",
    "sys.path.append('.')\n",
    "\n",
    "# Import pipeline modules\n",
    "from scripts.train import (Config, train_phase1, train_phase2, train_segmentation, \n",
    "                          explain_predictions, optimize_hyperparameters)\n",
    "from scripts.augmentations import (get_classification_transforms, get_segmentation_transforms, \n",
    "                                 AugmentationConfig)\n",
    "# NOTE: Configuration factories will be defined in this notebook (no import from scripts.config)\n",
    "\n",
    "# Verify CUDA availability\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA available: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "else:\n",
    "    print(\"CUDA not available - using CPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f78b4fca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning up environment\n",
      "✅ GPU memory cache cleared (8.6 GB available)\n",
      "Environment initialized successfully\n",
      "   Variables cleaned: 0\n",
      "   Memory optimization: Complete\n",
      "Ready for reproducible experiment execution\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Memory Management and Kernel Cleanup\n",
    "====================================\n",
    "\n",
    "Ensures clean execution environment by clearing previous variables and optimizing memory usage.\n",
    "This is particularly important for reproducible results in deep learning experiments.\n",
    "\"\"\"\n",
    "\n",
    "import gc\n",
    "\n",
    "# Print messages only once per kernel session\n",
    "SENTINEL = \"_CLEANUP_MESSAGES_PRINTED\"\n",
    "first_time = not globals().get(SENTINEL, False)\n",
    "\n",
    "is_torch = 'torch' in globals()\n",
    "is_gpu = is_torch and hasattr(globals()['torch'], 'cuda') and globals()['torch'].cuda.is_available() if is_torch else False\n",
    "\n",
    "if first_time:\n",
    "    print(\"Cleaning up environment\")\n",
    "\n",
    "# Clear potential variables from previous runs\n",
    "cleanup_variables = [\n",
    "    'att_weights', 'attention_weights', 'cam', 'grayscale_cam', \n",
    "    'img_mil', 'img_tensor', 'model', 'outputs', 'predicted',\n",
    "    'target_layer', 'target_layers', 'test_image', 'wrapper_model'\n",
    "]\n",
    "\n",
    "cleaned_count = 0\n",
    "for var_name in cleanup_variables:\n",
    "    if var_name in globals():\n",
    "        del globals()[var_name]\n",
    "        cleaned_count += 1\n",
    "\n",
    "# Force garbage collection\n",
    "gc.collect()\n",
    "\n",
    "# Clear GPU memory cache if available (always execute, only print once)\n",
    "if is_gpu:\n",
    "    globals()['torch'].cuda.empty_cache()\n",
    "\n",
    "if first_time:\n",
    "    if is_gpu:\n",
    "        gpu_memory = globals()['torch'].cuda.get_device_properties(0).total_memory / 1e9\n",
    "        print(f\"✅ GPU memory cache cleared ({gpu_memory:.1f} GB available)\")\n",
    "    else:\n",
    "        print(\"No GPU available or torch not imported; skipping GPU cache clear.\")\n",
    "    \n",
    "    print(f\"Environment initialized successfully\")\n",
    "    print(f\"   Variables cleaned: {cleaned_count}\")\n",
    "    print(f\"   Memory optimization: Complete\")\n",
    "    print(\"Ready for reproducible experiment execution\")\n",
    "    \n",
    "    # Set sentinel so subsequent runs are silent\n",
    "    globals()[SENTINEL] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "26e3b3ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Notebook-scoped configuration (no external import)\n",
    "=================================================\n",
    "\n",
    "Defines dataclasses for pipeline configuration and three presets: quick_test, default, production.\n",
    "These are adapted by training via coerce_to_train_config.\n",
    "\"\"\"\n",
    "\n",
    "from dataclasses import dataclass, field\n",
    "from pathlib import Path\n",
    "from typing import Optional\n",
    "\n",
    "@dataclass\n",
    "class DataConfigNB:\n",
    "    data_dir: str = \"data\"\n",
    "    annotations_dir: str = \"Annotations\"\n",
    "    output_dir: str = \"output\"\n",
    "    checkpoints_dir: str = \"checkpoints\"\n",
    "    patch_size: int = 512\n",
    "\n",
    "    def __post_init__(self):\n",
    "        Path(self.output_dir).mkdir(parents=True, exist_ok=True)\n",
    "        Path(self.checkpoints_dir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "@dataclass\n",
    "class ModelConfigNB:\n",
    "    num_classes: int = 2\n",
    "    batch_size: int = 16\n",
    "    learning_rate: float = 1e-4\n",
    "    num_epochs: int = 50\n",
    "    backbone: str = \"resnet50\"\n",
    "\n",
    "@dataclass\n",
    "class TrainingConfigNB:\n",
    "    device: str = \"auto\"  # \"cuda\" | \"cpu\" | \"auto\"\n",
    "    cross_validation_folds: int = 5\n",
    "    num_workers: int = 0\n",
    "    # New: patch sampling controls per phase\n",
    "    patches_per_slide_phase1: int = 100\n",
    "    patches_per_slide_phase2: int = 200\n",
    "    patches_per_slide_seg: int = 50\n",
    "    # Optional fast-mode overrides\n",
    "    fast_patches_per_slide_phase1: Optional[int] = 32\n",
    "    fast_patches_per_slide_phase2: Optional[int] = 64\n",
    "    fast_patches_per_slide_seg: Optional[int] = 16\n",
    "\n",
    "@dataclass\n",
    "class AugmentationConfigNB:\n",
    "    elastic_deform_prob: float = 0.3\n",
    "    stain_augment_prob: float = 0.5\n",
    "    use_otsu_tissue_mask: bool = True\n",
    "\n",
    "@dataclass\n",
    "class PipelineConfigNB:\n",
    "    data: DataConfigNB = field(default_factory=DataConfigNB)\n",
    "    model: ModelConfigNB = field(default_factory=ModelConfigNB)\n",
    "    training: TrainingConfigNB = field(default_factory=TrainingConfigNB)\n",
    "    augment: AugmentationConfigNB = field(default_factory=AugmentationConfigNB)\n",
    "\n",
    "def create_quick_test_config():\n",
    "    cfg = PipelineConfigNB()\n",
    "    cfg.model.batch_size = 4\n",
    "    cfg.model.num_epochs = 5\n",
    "    cfg.model.learning_rate = 3e-4\n",
    "    cfg.training.num_workers = 0  # Windows-friendly\n",
    "    cfg.data.patch_size = 256\n",
    "    # Lighter sampling for quick tests\n",
    "    cfg.training.patches_per_slide_phase1 = 48\n",
    "    cfg.training.patches_per_slide_phase2 = 96\n",
    "    cfg.training.patches_per_slide_seg = 24\n",
    "    return cfg\n",
    "\n",
    "def create_default_config():\n",
    "    cfg = PipelineConfigNB()\n",
    "    cfg.model.batch_size = 8\n",
    "    cfg.model.num_epochs = 30\n",
    "    cfg.model.learning_rate = 1e-4\n",
    "    cfg.training.num_workers = 0\n",
    "    cfg.data.patch_size = 512\n",
    "    # Balanced defaults\n",
    "    cfg.training.patches_per_slide_phase1 = 100\n",
    "    cfg.training.patches_per_slide_phase2 = 200\n",
    "    cfg.training.patches_per_slide_seg = 50\n",
    "    return cfg\n",
    "\n",
    "def create_production_config():\n",
    "    cfg = PipelineConfigNB()\n",
    "    cfg.model.batch_size = 1\n",
    "    cfg.model.num_epochs = 100\n",
    "    cfg.model.learning_rate = 5e-5\n",
    "    cfg.training.num_workers = 4\n",
    "    cfg.data.patch_size = 512\n",
    "    # Heavier sampling for robust training\n",
    "    cfg.training.patches_per_slide_phase1 = 256\n",
    "    cfg.training.patches_per_slide_phase2 = 512\n",
    "    cfg.training.patches_per_slide_seg = 128\n",
    "    return cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ef323793",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fast mode: True\n",
      "Batch caps per epoch (train/val): 4 / 2\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Fast mode toggle (notebook-friendly)\n",
    "===================================\n",
    "\n",
    "Enable this to speed up training in the notebook with fewer batches per epoch and smaller workloads.\n",
    "\"\"\"\n",
    "\n",
    "# Toggle fast mode for quick iteration\n",
    "POC_FAST_MODE = True   # Set to False for full runs\n",
    "\n",
    "# Optional batch caps per epoch when fast mode is enabled\n",
    "FAST_TRAIN_CAP = 4\n",
    "FAST_VAL_CAP = 2\n",
    "\n",
    "print(\"Fast mode:\", POC_FAST_MODE)\n",
    "print(\"Batch caps per epoch (train/val):\", (FAST_TRAIN_CAP if POC_FAST_MODE else None), \"/\", (FAST_VAL_CAP if POC_FAST_MODE else None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "53fb1e8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating dataset metadata\n",
      "================================\n",
      "Data directory: data\n",
      "SVS files: data\\SVS\n",
      "Annotations: data\\Annotations\n",
      "\n",
      "Processing 192 SVS files...\n",
      "\n",
      "Dataset statistics:\n",
      "  Total slides: 192\n",
      "  HER2- cases: 99\n",
      "  HER2+ cases: 93\n",
      "  Annotated slides: 187\n",
      "  Annotation coverage: 97.4%\n",
      "\n",
      "Sample metadata (first 3 rows):\n",
      "     slide_name label  has_annotation\n",
      "Her2Neg_Case_01 HER2-            True\n",
      "Her2Neg_Case_02 HER2-            True\n",
      "Her2Neg_Case_03 HER2-            True\n",
      "\n",
      "ROI annotation distribution:\n",
      "  HER2- with ROIs: 97\n",
      "  HER2+ with ROIs: 90\n",
      "  Class balance: 48.1% HER2+\n",
      "\n",
      "Metadata saved: data\\metadata.csv\n",
      "Dataset preparation complete\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Dataset metadata generation\n",
    "===========================\n",
    "\n",
    "Creates a CSV linking SVS whole-slide images with XML annotations. Enables ROI-supervised training by mapping pathologist annotations to slides.\n",
    "\"\"\"\n",
    "\n",
    "print(\"Generating dataset metadata\")\n",
    "print(\"=\" * 32)\n",
    "\n",
    "# Define data paths\n",
    "data_dir = Path(\"data\")\n",
    "svs_dir = data_dir / \"SVS\"\n",
    "annotations_dir = data_dir / Path(\"Annotations\")\n",
    "metadata_file = data_dir / \"metadata.csv\"\n",
    "\n",
    "print(f\"Data directory: {data_dir}\")\n",
    "print(f\"SVS files: {svs_dir}\")\n",
    "print(f\"Annotations: {annotations_dir}\")\n",
    "\n",
    "# Validate directory structure\n",
    "if not svs_dir.exists():\n",
    "    print(f\"Warning: SVS directory not found at {svs_dir}\")\n",
    "if not annotations_dir.exists():\n",
    "    print(f\"Warning: Annotations directory not found at {annotations_dir}\")\n",
    "\n",
    "# Collect slide information\n",
    "slides_data = []\n",
    "her2_neg_count = 0\n",
    "her2_pos_count = 0\n",
    "annotated_count = 0\n",
    "\n",
    "# Process all SVS files\n",
    "svs_files = list(svs_dir.glob(\"*.svs\")) if svs_dir.exists() else []\n",
    "print(f\"\\nProcessing {len(svs_files)} SVS files...\")\n",
    "\n",
    "for svs_file in svs_files:\n",
    "    slide_name = svs_file.stem\n",
    "    \n",
    "    # Extract HER2 status from filename\n",
    "    if slide_name.startswith(\"Her2Neg\"):\n",
    "        her2_status = 0\n",
    "        label = \"HER2-\"\n",
    "        her2_neg_count += 1\n",
    "    elif slide_name.startswith(\"Her2Pos\"):\n",
    "        her2_status = 1\n",
    "        label = \"HER2+\"\n",
    "        her2_pos_count += 1\n",
    "    else:\n",
    "        print(f\"Unrecognized slide naming: {slide_name}\")\n",
    "        continue\n",
    "    \n",
    "    # Check for corresponding annotation\n",
    "    annotation_file = annotations_dir / f\"{slide_name}.xml\"\n",
    "    has_annotation = annotation_file.exists()\n",
    "    if has_annotation:\n",
    "        annotated_count += 1\n",
    "    \n",
    "    slide_info = {\n",
    "        'slide_name': slide_name,\n",
    "        'slide_path': str(svs_file),\n",
    "        'her2_status': her2_status,\n",
    "        'label': label,\n",
    "        'has_annotation': has_annotation,\n",
    "        'annotation_path': str(annotation_file) if has_annotation else None\n",
    "    }\n",
    "    slides_data.append(slide_info)\n",
    "\n",
    "# Create structured DataFrame\n",
    "df = pd.DataFrame(slides_data)\n",
    "df = df.sort_values('slide_name').reset_index(drop=True)\n",
    "\n",
    "# Save metadata\n",
    "df.to_csv(metadata_file, index=False)\n",
    "\n",
    "# Display dataset statistics\n",
    "print(f\"\\nDataset statistics:\")\n",
    "print(f\"  Total slides: {len(df)}\")\n",
    "print(f\"  HER2- cases: {her2_neg_count}\")\n",
    "print(f\"  HER2+ cases: {her2_pos_count}\")\n",
    "print(f\"  Annotated slides: {annotated_count}\")\n",
    "print(f\"  Annotation coverage: {(annotated_count/len(df)*100):.1f}%\" if len(df) > 0 else \"  Annotation coverage: 0%\")\n",
    "\n",
    "# Display sample data\n",
    "if len(df) > 0:\n",
    "    print(f\"\\nSample metadata (first 3 rows):\")\n",
    "    print(df[['slide_name', 'label', 'has_annotation']].head(3).to_string(index=False))\n",
    "\n",
    "# Annotation distribution analysis\n",
    "if annotated_count > 0:\n",
    "    annotated_df = df[df['has_annotation'] == True]\n",
    "    roi_neg = len(annotated_df[annotated_df['her2_status'] == 0])\n",
    "    roi_pos = len(annotated_df[annotated_df['her2_status'] == 1])\n",
    "    \n",
    "    print(f\"\\nROI annotation distribution:\")\n",
    "    print(f\"  HER2- with ROIs: {roi_neg}\")\n",
    "    print(f\"  HER2+ with ROIs: {roi_pos}\")\n",
    "    print(f\"  Class balance: {(roi_pos/(roi_neg+roi_pos)*100):.1f}% HER2+\" if (roi_neg+roi_pos) > 0 else \"  Class balance: No data\")\n",
    "\n",
    "print(f\"\\nMetadata saved: {metadata_file}\")\n",
    "print(\"Dataset preparation complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4fbba63d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running diagnostics (preflight checks)…\n",
      "[diagnostics] No prior config found; using default TrainConfig()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Diagnostics: 100%|██████████| 8/8 [00:26<00:00,  3.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Diagnostics Summary\n",
      "====================\n",
      "Python: 3.12.3 | packaged by conda-forge | (main, Apr 15 2024, 18:20:11) [MSC v.1938 64 bit (AMD64)]\n",
      "PyTorch: 2.8.0+cu128 | CUDA: Yes\n",
      "  GPU: NVIDIA GeForce RTX 4060 Laptop GPU | VRAM: 8.59 GB | bf16: Yes\n",
      "torch.compile: Yes | Triton: Yes | compile_probe: Yes\n",
      "Optional deps:\n",
      "  - openslide: Yes\n",
      "  - monai: Yes\n",
      "  - wandb: Yes\n",
      "  - tensorboard: Yes\n",
      "  - pytorch-gradcam: Yes\n",
      "  - sklearn: Yes\n",
      "  - opencv: Yes\n",
      "  - triton: Yes\n",
      "bf16->NumPy safe path: Yes\n",
      "Data paths: data=Yes, ann=Yes, svs=Yes (count=192)\n",
      "Tiny forward probes:\n",
      "  AttentionMIL: Yes\n",
      "  SegmentationUNet: Yes\n",
      "Grad-CAM smoke:\n",
      "  explain fn: Yes | pkg: Yes\n",
      "  checkpoint: checkpoints\\best_model.pth | executed: Yes\n",
      "Overall OK: Yes\n",
      "Saved diagnostics to output\\logs\\diagnostics.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Diagnostics: preflight checks (optional to run before training)\n",
    "print(\"Running diagnostics (preflight checks)…\")\n",
    "from scripts import diagnostics as diag\n",
    "import importlib\n",
    "\n",
    "# Reload to pick up recent changes to diagnostics module\n",
    "try:\n",
    "    diag = importlib.reload(diag)\n",
    "except Exception as e:\n",
    "    print(f\"[diagnostics] reload note: {e}\")\n",
    "\n",
    "# Resolve a config object robustly even if prior cells weren't run\n",
    "try:\n",
    "    cfg = legacy_config  # from earlier cells, if present\n",
    "except NameError:\n",
    "    try:\n",
    "        cfg = config  # pipeline config from earlier cells\n",
    "    except NameError:\n",
    "        from scripts.train import Config as TrainConfig\n",
    "        cfg = TrainConfig()\n",
    "        print(\"[diagnostics] No prior config found; using default TrainConfig()\")\n",
    "\n",
    "# Persist to legacy_config so downstream cells can rely on it\n",
    "legacy_config = cfg\n",
    "\n",
    "try:\n",
    "    # Enable a CPU-only compile probe on Windows to surface compile_probe results\n",
    "    results = diag.run_all_checks(legacy_config, try_compile_probe_on_windows=True)\n",
    "    diag._print_human(results)\n",
    "    from pathlib import Path\n",
    "    import json\n",
    "    out_dir = Path(\"output\") / \"logs\"\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "    with open(out_dir / \"diagnostics.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(results, f, indent=2, default=str)\n",
    "    print(f\"Saved diagnostics to {out_dir / 'diagnostics.json'}\")\n",
    "except Exception as e:\n",
    "    print(f\"Diagnostics failed: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "95f50268",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance settings:\n",
      "{'USE_TORCH_COMPILE': True, 'USE_CHANNELS_LAST': True, 'AMP_DTYPE': 'torch.bfloat16', 'USE_FUSED_ADAMW': True, 'ZERO_SET_TO_NONE': True}\n"
     ]
    }
   ],
   "source": [
    "# Enable and display performance optimization toggles for training\n",
    "from scripts.train import Config as TrainConfig\n",
    "import importlib.util\n",
    "\n",
    "# Adopt legacy_config if present, otherwise create a fresh TrainConfig\n",
    "cfg = legacy_config if 'legacy_config' in globals() else TrainConfig()\n",
    "\n",
    "# Detect Triton availability (required for torch.compile on CUDA/Inductor)\n",
    "def _has_triton():\n",
    "    try:\n",
    "        return importlib.util.find_spec(\"triton\") is not None\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "# Decide whether to enable torch.compile\n",
    "if torch.cuda.is_available() and not _has_triton():\n",
    "    cfg.USE_TORCH_COMPILE = False\n",
    "    print(\"[compile] Triton not found; disabling torch.compile on CUDA to avoid Inductor error\")\n",
    "else:\n",
    "    cfg.USE_TORCH_COMPILE = True\n",
    "\n",
    "# Other performance toggles\n",
    "cfg.USE_CHANNELS_LAST = True\n",
    "cfg.USE_FUSED_ADAMW = True\n",
    "cfg.ZERO_SET_TO_NONE = True\n",
    "\n",
    "# Prefer bf16 when supported, else fp16\n",
    "try:\n",
    "    if hasattr(torch.cuda, 'is_bf16_supported') and torch.cuda.is_bf16_supported():\n",
    "        cfg.AMP_DTYPE = torch.bfloat16\n",
    "    else:\n",
    "        cfg.AMP_DTYPE = torch.float16\n",
    "except Exception:\n",
    "    cfg.AMP_DTYPE = torch.float16\n",
    "\n",
    "# Print the active optimization settings for visibility\n",
    "print(\"Performance settings:\")\n",
    "print({\n",
    "    'USE_TORCH_COMPILE': cfg.USE_TORCH_COMPILE,\n",
    "    'USE_CHANNELS_LAST': cfg.USE_CHANNELS_LAST,\n",
    "    'AMP_DTYPE': str(cfg.AMP_DTYPE),\n",
    "    'USE_FUSED_ADAMW': cfg.USE_FUSED_ADAMW,\n",
    "    'ZERO_SET_TO_NONE': cfg.ZERO_SET_TO_NONE,\n",
    "})\n",
    "\n",
    "# Persist back to legacy_config so downstream cells use the same object\n",
    "legacy_config = cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d3d730e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights & Biases: ROI-supervised training runs\n",
      "================================================\n",
      "Project: https://wandb.ai/thanakornbua/her2-breast-cancer\n",
      "Runs (illustrative):\n",
      "  • Phase 1 (ROI classification): Phase1_ROI_Supervised_fold0\n",
      "  • Phase 2 (MIL): Phase2_MIL_FineTuning_fold0\n",
      "  • Phase 3 (Segmentation): Phase3_Segmentation_fold0\n",
      "\n",
      "Monitored signals:\n",
      "  • Loss (train/val), LR schedule, early stopping\n",
      "  • Accuracy, Precision/Recall/F1, ROC-AUC\n",
      "  • Confusion matrix and classification report\n",
      "  • ROI coverage and patch sampling stats\n",
      "  • Segmentation Dice/IoU (if enabled)\n"
     ]
    }
   ],
   "source": [
    "# Weights & Biases: links and monitored signals (concise)\n",
    "print(\"Weights & Biases: ROI-supervised training runs\")\n",
    "print(\"=\" * 48)\n",
    "\n",
    "# Replace with your project URL if different\n",
    "print(\"Project: https://wandb.ai/thanakornbua/her2-breast-cancer\")\n",
    "print(\"Runs (illustrative):\")\n",
    "print(\"  • Phase 1 (ROI classification): Phase1_ROI_Supervised_fold0\")\n",
    "print(\"  • Phase 2 (MIL): Phase2_MIL_FineTuning_fold0\")\n",
    "print(\"  • Phase 3 (Segmentation): Phase3_Segmentation_fold0\")\n",
    "\n",
    "print(\"\\nMonitored signals:\")\n",
    "print(\"  • Loss (train/val), LR schedule, early stopping\")\n",
    "print(\"  • Accuracy, Precision/Recall/F1, ROC-AUC\")\n",
    "print(\"  • Confusion matrix and classification report\")\n",
    "print(\"  • ROI coverage and patch sampling stats\")\n",
    "print(\"  • Segmentation Dice/IoU (if enabled)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38a7c2c9",
   "metadata": {},
   "source": [
    "## Phase 1 — ROI-supervised classification\n",
    "Run Phase 1 to train a ResNet-50 AttentionMIL strictly on annotated ROIs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f4d9507",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Phase 1: ROI-supervised classification\n",
      "================================================\n",
      "[FAST] Fast mode enabled: INPUT_SIZE=256, EPOCHS(P1/P2)=5/5, PATCHES/SLIDE(P1/P2/Seg)=32/64/16, SEG_PATCH=192, OTSU=False, CKPT=False\n",
      "Config summary:\n",
      "{'PATCH_SIZE': 512, 'BATCH_SIZE': 16, 'N_FOLDS': 5, 'PATCHES_PER_SLIDE_P1': 32, 'PATCHES_PER_SLIDE_P2': 64, 'PATCHES_PER_SLIDE_SEG': 16, 'USE_TORCH_COMPILE': True, 'USE_CHANNELS_LAST': True, 'AMP_DTYPE': 'torch.bfloat16', 'USE_FUSED_ADAMW': True, 'ZERO_SET_TO_NONE': True, 'REQUIRE_ROI_FOR_PHASE1': True, 'ALLOW_FALLBACK_OUTSIDE_ROI': False, 'FAST_MAX_TRAIN_BATCHES_PER_EPOCH': 4, 'FAST_MAX_VAL_BATCHES_PER_EPOCH': 2}\n",
      "[Data] Slides with ROI annotations: 187/192\n",
      "[ROI] Phase 1 ROI-only mode: filtered train 153->148, val 39->39\n",
      "[Data] Slides with ROI annotations: 187/192\n",
      "[ROI] Phase 1 ROI-only mode: filtered train 153->148, val 39->39\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: You can find your API key in your browser here: http://localhost:8080/authorize?ref=models\n",
      "wandb: Paste an API key from your profile and hit enter:wandb: WARNING If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "wandb: WARNING Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "wandb: Appending key for localhost:8080 to your netrc file: C:\\Users\\tanth\\_netrc\n",
      "wandb: Currently logged in as: thanakornbua to http://localhost:8080. Use `wandb login --relogin` to force relogin\n"
     ]
    }
   ],
   "source": [
    "# Phase 1: ROI-supervised classification\n",
    "print(\"Phase 1: ROI-supervised classification\")\n",
    "print(\"=\" * 48)\n",
    "\n",
    "try:\n",
    "    # Coerce config for training functions (accepts PipelineConfig)\n",
    "    from scripts.train import coerce_to_train_config\n",
    "    legacy_config = coerce_to_train_config(legacy_config)\n",
    "\n",
    "    # Ensure strict ROI flags are enabled\n",
    "    legacy_config.REQUIRE_ROI_FOR_PHASE1 = True\n",
    "    legacy_config.ALLOW_FALLBACK_OUTSIDE_ROI = False\n",
    "    legacy_config.ROI_MAX_SAMPLING_ATTEMPTS = 20\n",
    "    \n",
    "    # Use fast mode toggle from notebook\n",
    "    from scripts.train import apply_fast_mode_overrides\n",
    "    if 'POC_FAST_MODE' in globals() and POC_FAST_MODE:\n",
    "        legacy_config.FAST_MODE = True\n",
    "        legacy_config.FAST_IGNORE_CHECKPOINTS = True\n",
    "        # Cap batches if provided\n",
    "        if 'FAST_TRAIN_CAP' in globals():\n",
    "            legacy_config.FAST_MAX_TRAIN_BATCHES_PER_EPOCH = FAST_TRAIN_CAP\n",
    "        if 'FAST_VAL_CAP' in globals():\n",
    "            legacy_config.FAST_MAX_VAL_BATCHES_PER_EPOCH = FAST_VAL_CAP\n",
    "        apply_fast_mode_overrides(legacy_config)\n",
    "    \n",
    "    # Safety: disable torch.compile on CUDA if Triton is missing\n",
    "    import importlib.util\n",
    "    if getattr(legacy_config, 'USE_TORCH_COMPILE', False) and torch.cuda.is_available() and importlib.import_module:\n",
    "        if importlib.util.find_spec('triton') is None:\n",
    "            print(\"[compile] Triton missing at runtime; disabling torch.compile for this session\")\n",
    "            legacy_config.USE_TORCH_COMPILE = False\n",
    "    \n",
    "    # Display key settings (robust to both config styles)\n",
    "    print(\"Config summary:\")\n",
    "    summary_fields = {\n",
    "        'PATCH_SIZE': getattr(legacy_config, 'PATCH_SIZE', getattr(getattr(legacy_config, 'data', object()), 'patch_size', 'n/a')),\n",
    "        'BATCH_SIZE': getattr(legacy_config, 'BATCH_SIZE', getattr(getattr(legacy_config, 'model', object()), 'batch_size', 'n/a')),\n",
    "        'N_FOLDS': getattr(legacy_config, 'N_FOLDS', getattr(getattr(legacy_config, 'training', object()), 'cross_validation_folds', 'n/a')),\n",
    "        'PATCHES_PER_SLIDE_P1': getattr(legacy_config, 'PATCHES_PER_SLIDE_PHASE1', getattr(getattr(legacy_config, 'training', object()), 'patches_per_slide_phase1', 'n/a')),\n",
    "        'PATCHES_PER_SLIDE_P2': getattr(legacy_config, 'PATCHES_PER_SLIDE_PHASE2', getattr(getattr(legacy_config, 'training', object()), 'patches_per_slide_phase2', 'n/a')),\n",
    "        'PATCHES_PER_SLIDE_SEG': getattr(legacy_config, 'PATCHES_PER_SLIDE_SEG', getattr(getattr(legacy_config, 'training', object()), 'patches_per_slide_seg', 'n/a')),\n",
    "        'USE_TORCH_COMPILE': getattr(legacy_config, 'USE_TORCH_COMPILE', False),\n",
    "        'USE_CHANNELS_LAST': getattr(legacy_config, 'USE_CHANNELS_LAST', False),\n",
    "        'AMP_DTYPE': str(getattr(legacy_config, 'AMP_DTYPE', 'n/a')),\n",
    "        'USE_FUSED_ADAMW': getattr(legacy_config, 'USE_FUSED_ADAMW', False),\n",
    "        'ZERO_SET_TO_NONE': getattr(legacy_config, 'ZERO_SET_TO_NONE', True),\n",
    "        'REQUIRE_ROI_FOR_PHASE1': getattr(legacy_config, 'REQUIRE_ROI_FOR_PHASE1', True),\n",
    "        'ALLOW_FALLBACK_OUTSIDE_ROI': getattr(legacy_config, 'ALLOW_FALLBACK_OUTSIDE_ROI', False),\n",
    "        'FAST_MAX_TRAIN_BATCHES_PER_EPOCH': getattr(legacy_config, 'MAX_TRAIN_BATCHES_PER_EPOCH', None),\n",
    "        'FAST_MAX_VAL_BATCHES_PER_EPOCH': getattr(legacy_config, 'MAX_VAL_BATCHES_PER_EPOCH', None),\n",
    "    }\n",
    "    print(summary_fields)\n",
    "    \n",
    "    # Run Phase 1 for a chosen fold (default 0)\n",
    "    fold = 0\n",
    "    from torch.utils.tensorboard import SummaryWriter\n",
    "    from pathlib import Path\n",
    "    tb_dir = Path(legacy_config.LOG_DIR) / \"tensorboard\"\n",
    "    tb_dir.mkdir(parents=True, exist_ok=True)\n",
    "    writer = SummaryWriter(log_dir=str(tb_dir / f\"phase1_fold{fold}\"))\n",
    "    \n",
    "    best_auc = train_phase1(legacy_config, fold=fold, writer=writer)\n",
    "    print(f\"Phase 1 completed. Best AUC: {best_auc:.4f}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Phase 1 failed: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ece1f24",
   "metadata": {},
   "source": [
    "## Phase 2 — Multiple instance learning (MIL) fine-tuning\n",
    "\n",
    "Objective: fine-tune an attention-based MIL classifier using slide-level labels, initialized from Phase 1 ROI-supervised features.\n",
    "\n",
    "- Initialization: load the best Phase 1 checkpoint; freeze early layers; tune the attention head and classifier.\n",
    "- Data: group ROI-derived patches into bags; optional tissue masking to exclude background.\n",
    "- Outputs: best-performing MIL checkpoint, attention visualizations, and tracked metrics.\n",
    "- Metrics: bag-level ROC-AUC, precision/recall/F1, confusion matrix; learning dynamics.\n",
    "- Reproducibility: all configs and artifacts are versioned; seeds fixed where supported."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cc23247c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Phase 2: MIL fine-tuning with frozen backbone\n",
      "================================================\n",
      "Configuration:\n",
      "Phase 2 training failed: 'PipelineConfig' object has no attribute 'EPOCHS_PHASE2'\n",
      "Phase 2 training session complete.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\tanth\\AppData\\Local\\Temp\\ipykernel_49696\\3930901177.py\", line 18, in <module>\n",
      "    print(f\"  Epochs: {legacy_config.EPOCHS_PHASE2}\")\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: 'PipelineConfig' object has no attribute 'EPOCHS_PHASE2'\n"
     ]
    }
   ],
   "source": [
    "# Phase 2: Multiple instance learning (MIL) fine-tuning\n",
    "\n",
    "print(\"Phase 2: MIL fine-tuning with frozen backbone\")\n",
    "print(\"=\" * 48)\n",
    "\n",
    "try:\n",
    "    # Ensure fast mode carries over to phase 2 as well\n",
    "    try:\n",
    "        from scripts.train import apply_fast_mode_overrides\n",
    "        if 'POC_FAST_MODE' in globals() and POC_FAST_MODE:\n",
    "            legacy_config.FAST_MODE = True\n",
    "            legacy_config.FAST_IGNORE_CHECKPOINTS = True\n",
    "            apply_fast_mode_overrides(legacy_config)\n",
    "    except Exception as e:\n",
    "        print(f\"[FAST] Note: could not re-apply fast mode for Phase 2: {e}\")\n",
    "\n",
    "    print(\"Configuration:\")\n",
    "    print(f\"  Epochs: {legacy_config.EPOCHS_PHASE2}\")\n",
    "    print(f\"  Learning rate: {legacy_config.LR_PHASE2}\")\n",
    "    print(f\"  Patches per slide: {legacy_config.PATCHES_PER_SLIDE_PHASE2}\")\n",
    "    print(f\"  Otsu tissue masking: {'Enabled' if legacy_config.USE_OTSU_TISSUE_MASK else 'Disabled'}\")\n",
    "    print(\"  Strategy: Freeze early layers; fine-tune attention and classifier\")\n",
    "\n",
    "    # Check if Phase 1 model exists\n",
    "    phase1_model_path = legacy_config.CHECKPOINT_DIR / \"phase1_fold0_best.pth\"\n",
    "    if not phase1_model_path.exists():\n",
    "        print(\"Phase 1 model not found. Skipping Phase 2.\")\n",
    "        print(\"Run Phase 1 training first to generate the initialization model.\")\n",
    "    else:\n",
    "        print(\"Starting MIL fine-tuning…\")\n",
    "        print(\"Using ROI-trained Phase 1 model for initialization.\")\n",
    "\n",
    "        # Run Phase 2 training (without TensorBoard writer)\n",
    "        train_phase2(legacy_config, fold=0, writer=None)\n",
    "\n",
    "        print(\"MIL fine-tuning completed.\")\n",
    "\n",
    "        # Generate explanations for Phase 2 model\n",
    "        phase2_model_path = legacy_config.CHECKPOINT_DIR / \"phase2_fold0_best.pth\"\n",
    "        if phase2_model_path.exists():\n",
    "            print(\"Generating MIL attention/Grad-CAM visualizations…\")\n",
    "            explain_predictions(legacy_config, str(phase2_model_path), fold=0, num_samples=3)\n",
    "            print(\"MIL explanations generated.\")\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    print(\"Phase 2 training interrupted by user.\")\n",
    "except Exception as e:\n",
    "    print(f\"Phase 2 training failed: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "finally:\n",
    "    print(\"Phase 2 training session complete.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "047c374d",
   "metadata": {},
   "source": [
    "## Phase 3 — ROI-derived segmentation\n",
    "\n",
    "Objective: train a U-Net–based segmentation model to delineate tissue regions using masks derived from ROI annotations.\n",
    "\n",
    "- Architecture: U-Net (ResNet backbone).\n",
    "- Data: convert ROI polygons to binary masks; use patch-based sampling.\n",
    "- Augmentation: elastic deformation and other MONAI transforms (configurable).\n",
    "- Outputs: best-performing segmentation checkpoint and selected overlays.\n",
    "- Metrics: Dice, IoU, pixel accuracy, and per-class breakdown.\n",
    "- Notes: Segmentation complements classification by localizing ROI-consistent patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fec0357d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Phase 3: Segmentation training with U-Net\n",
      "Segmentation masks are derived from ROI annotations\n",
      "================================================\n",
      "Segmentation ROI-only: Enabled (patches must contain positive mask pixels)\n",
      "Configuration:\n",
      "  Architecture: U-Net with ResNet backbone\n",
      "Segmentation training failed: 'PipelineConfig' object has no attribute 'PATCH_SIZE_SEG'\n",
      "Segmentation training session complete.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\tanth\\AppData\\Local\\Temp\\ipykernel_49696\\3275890057.py\", line 31, in <module>\n",
      "    print(f\"  Patch size: {legacy_config.PATCH_SIZE_SEG}\")\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: 'PipelineConfig' object has no attribute 'PATCH_SIZE_SEG'\n"
     ]
    }
   ],
   "source": [
    "# Phase 3: Segmentation training (ROI-derived masks)\n",
    "print(\"Phase 3: Segmentation training with U-Net\")\n",
    "print(\"Segmentation masks are derived from ROI annotations\")\n",
    "print(\"=\" * 48)\n",
    "\n",
    "try:\n",
    "    # Ensure fast mode also affects segmentation stage\n",
    "    try:\n",
    "        from scripts.train import apply_fast_mode_overrides\n",
    "        if 'POC_FAST_MODE' in globals() and POC_FAST_MODE:\n",
    "            legacy_config.FAST_MODE = True\n",
    "            legacy_config.FAST_IGNORE_CHECKPOINTS = True\n",
    "            apply_fast_mode_overrides(legacy_config)\n",
    "    except Exception as e:\n",
    "        print(f\"[FAST] Note: could not re-apply fast mode for Segmentation: {e}\")\n",
    "\n",
    "    # Safety: disable torch.compile if Triton is missing on CUDA\n",
    "    import importlib.util\n",
    "    if getattr(legacy_config, 'USE_TORCH_COMPILE', False) and torch.cuda.is_available() and importlib.util.find_spec('triton') is None:\n",
    "        print(\"[compile] Triton missing at runtime; disabling torch.compile for Segmentation\")\n",
    "        legacy_config.USE_TORCH_COMPILE = False\n",
    "\n",
    "    # Enforce ROI-only segmentation patches (positive-mask requirement)\n",
    "    legacy_config.REQUIRE_ROI_FOR_SEGMENTATION = True\n",
    "    legacy_config.REQUIRE_POSITIVE_MASK_PATCHES = True\n",
    "    legacy_config.POS_MASK_MAX_ATTEMPTS = max(getattr(legacy_config, 'POS_MASK_MAX_ATTEMPTS', 20), 20)\n",
    "    print(\"Segmentation ROI-only: Enabled (patches must contain positive mask pixels)\")\n",
    "\n",
    "    print(\"Configuration:\")\n",
    "    print(f\"  Architecture: U-Net with ResNet backbone\")\n",
    "    print(f\"  Patch size: {legacy_config.PATCH_SIZE_SEG}\")\n",
    "    print(f\"  Batch size: {legacy_config.BATCH_SIZE}\")\n",
    "    print(f\"  Elastic deformation: {legacy_config.ELASTIC_DEFORM_PROB * 100}% probability\")\n",
    "    print(\"  Augmentations: MONAI transforms\")\n",
    "    print(\"  Task: Binary segmentation from ROI-derived masks\")\n",
    "\n",
    "    print(\"\\nStarting segmentation training…\")\n",
    "    print(\"Using ROI annotations to generate ground truth masks.\")\n",
    "\n",
    "    # Run Segmentation training (without TensorBoard writer)\n",
    "    train_segmentation(legacy_config, fold=0, writer=None)\n",
    "\n",
    "    print(\"Segmentation training completed.\")\n",
    "\n",
    "    # Check for segmentation model\n",
    "    seg_model_path = legacy_config.CHECKPOINT_DIR / \"segmentation_fold0_best.pth\"\n",
    "    if seg_model_path.exists():\n",
    "        print(\"Segmentation model saved.\")\n",
    "        print(f\"  Model path: {seg_model_path}\")\n",
    "        print(\"  Model trained to segment ROI-consistent regions.\")\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    print(\"Segmentation training interrupted by user.\")\n",
    "except Exception as e:\n",
    "    print(f\"Segmentation training failed: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "finally:\n",
    "    print(\"Segmentation training session complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d2b8f802",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔧 Hyperparameter Optimization with Bayesian Optimization\n",
      "============================================================\n",
      "⚠️ Hyperparameter optimization is disabled\n",
      "   Set ENABLE_OPTIMIZATION = True to run optimization\n",
      "   Note: This may take considerable time depending on N_TRIALS\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameter Optimization with Optuna\n",
    "print(\"🔧 Hyperparameter Optimization with Bayesian Optimization\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Configuration for optimization\n",
    "ENABLE_OPTIMIZATION = False  # Set to True to run optimization\n",
    "N_TRIALS = 20  # Reduced for notebook demo\n",
    "\n",
    "if ENABLE_OPTIMIZATION:\n",
    "    try:\n",
    "        print(\"🎯 Optuna configuration:\")\n",
    "        print(f\"   - Number of trials: {N_TRIALS}\")\n",
    "        print(\"   - Sampler: TPE (Tree-structured Parzen Estimator)\")\n",
    "        print(\"   - Objective: Maximize F1-score\")\n",
    "        print(\"   - Pruning: Enabled for early stopping\")\n",
    "        \n",
    "        print(\"🚀 Starting hyperparameter optimization...\")\n",
    "        \n",
    "        # Run Optuna optimization\n",
    "        optimized_config = optimize_hyperparameters(legacy_config, n_trials=N_TRIALS)\n",
    "        \n",
    "        print(\"✅ Hyperparameter optimization completed!\")\n",
    "        \n",
    "        # Display optimized parameters\n",
    "        print(\"\\n📊 Optimized Hyperparameters:\")\n",
    "        print(f\"   - Learning Rate: {optimized_config.LR_PHASE1}\")\n",
    "        print(f\"   - Batch Size: {optimized_config.BATCH_SIZE}\")\n",
    "        print(f\"   - Weight Decay: {optimized_config.WEIGHT_DECAY}\")\n",
    "        \n",
    "        # Save optimized configuration\n",
    "        import json\n",
    "        optimized_params = {\n",
    "            'learning_rate': optimized_config.LR_PHASE1,\n",
    "            'batch_size': optimized_config.BATCH_SIZE,\n",
    "            'weight_decay': optimized_config.WEIGHT_DECAY,\n",
    "            'trials_completed': N_TRIALS\n",
    "        }\n",
    "        \n",
    "        with open('optimized_config_notebook.json', 'w') as f:\n",
    "            json.dump(optimized_params, f, indent=2, default=str)\n",
    "        \n",
    "        print(\"💾 Optimized configuration saved to: optimized_config_notebook.json\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Hyperparameter optimization failed: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "else:\n",
    "    print(\"⚠️ Hyperparameter optimization is disabled\")\n",
    "    print(\"   Set ENABLE_OPTIMIZATION = True to run optimization\")\n",
    "    print(\"   Note: This may take considerable time depending on N_TRIALS\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0aa63f78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results analysis and visualization\n",
      "ROI-supervised training results\n",
      "====================================\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'PipelineConfig' object has no attribute 'CHECKPOINT_DIR'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 18\u001b[0m\n\u001b[0;32m     15\u001b[0m sns\u001b[38;5;241m.\u001b[39mset_palette(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhusl\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# Check for training results\u001b[39;00m\n\u001b[1;32m---> 18\u001b[0m checkpoint_dir \u001b[38;5;241m=\u001b[39m Path(legacy_config\u001b[38;5;241m.\u001b[39mCHECKPOINT_DIR)\n\u001b[0;32m     19\u001b[0m log_dir \u001b[38;5;241m=\u001b[39m Path(legacy_config\u001b[38;5;241m.\u001b[39mLOG_DIR)\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mChecking available model checkpoints…\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'PipelineConfig' object has no attribute 'CHECKPOINT_DIR'"
     ]
    }
   ],
   "source": [
    "# Results analysis and visualization (ROI-focused)\n",
    "print(\"Results analysis and visualization\")\n",
    "print(\"ROI-supervised training results\")\n",
    "print(\"=\" * 36)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Set plotting style\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Check for training results\n",
    "checkpoint_dir = Path(legacy_config.CHECKPOINT_DIR)\n",
    "log_dir = Path(legacy_config.LOG_DIR)\n",
    "\n",
    "print(\"Checking available model checkpoints…\")\n",
    "\n",
    "# Display available models\n",
    "model_files = list(checkpoint_dir.glob(\"*.pth\"))\n",
    "if model_files:\n",
    "    print(f\"Found {len(model_files)} checkpoint(s):\")\n",
    "    for model_file in model_files:\n",
    "        size_mb = model_file.stat().st_size / (1024 * 1024)\n",
    "        print(f\"  - {model_file.name} ({size_mb:.1f} MB)\")\n",
    "        if \"phase1\" in model_file.name:\n",
    "            print(\"    Type: ROI-supervised classification model\")\n",
    "        elif \"phase2\" in model_file.name:\n",
    "            print(\"    Type: MIL model (initialized from ROI-trained base)\")\n",
    "        elif \"segmentation\" in model_file.name:\n",
    "            print(\"    Type: Segmentation model (ROI-derived masks)\")\n",
    "else:\n",
    "    print(\"No checkpoints found. Run training phases first.\")\n",
    "\n",
    "# Check for training logs\n",
    "if log_dir.exists():\n",
    "    log_files = list(log_dir.glob(\"*.log\"))\n",
    "    if log_files:\n",
    "        print(f\"\\nFound {len(log_files)} training log file(s)\")\n",
    "        for log_file in log_files:\n",
    "            print(f\"  - {log_file.name}\")\n",
    "    else:\n",
    "        print(\"\\nNo log files found.\")\n",
    "\n",
    "# Display augmentation examples (synthetic ROI-like image)\n",
    "print(\"\\nTesting augmentation pipeline (synthetic example)…\")\n",
    "try:\n",
    "    from scripts.augmentations import get_classification_transforms\n",
    "    try:\n",
    "        from scripts.augmentations import elastic_deformation, NativeStainNormalizer\n",
    "        native_stain_available = True\n",
    "    except ImportError:\n",
    "        print(\"Native stain normalization not available\")\n",
    "        native_stain_available = False\n",
    "\n",
    "    # Create a sample ROI-like image\n",
    "    sample_image = np.random.randint(50, 200, (256, 256, 3), dtype=np.uint8)\n",
    "    for _ in range(15):\n",
    "        x, y = np.random.randint(20, 236, 2)\n",
    "        size = np.random.randint(4, 12)\n",
    "        Y, X = np.ogrid[:256, :256]\n",
    "        mask = (X - x)**2 + (Y - y)**2 <= size**2\n",
    "        sample_image[mask, 0] = np.clip(120 + np.random.randint(-20, 20), 0, 255)\n",
    "        sample_image[mask, 1] = np.clip(100 + np.random.randint(-15, 15), 0, 255)\n",
    "        sample_image[mask, 2] = np.clip(180 + np.random.randint(-15, 15), 0, 255)\n",
    "\n",
    "    results = [sample_image]\n",
    "    titles = [\"Original ROI-like image\"]\n",
    "\n",
    "    if native_stain_available:\n",
    "        try:\n",
    "            elastic_result = elastic_deformation(sample_image, alpha=100, sigma=10)\n",
    "            results.append(elastic_result)\n",
    "            titles.append(\"Elastic deformation\")\n",
    "\n",
    "            normalizer = NativeStainNormalizer()\n",
    "            stain_result = normalizer.fit_transform([sample_image])[0]\n",
    "            results.append(stain_result)\n",
    "            titles.append(\"Native H&E normalization\")\n",
    "        except Exception as e:\n",
    "            print(f\"Augmentation error: {e}\")\n",
    "\n",
    "    n_images = len(results)\n",
    "    fig, axes = plt.subplots(1, n_images, figsize=(5*n_images, 5))\n",
    "    if n_images == 1:\n",
    "        axes = [axes]\n",
    "    for i, (img, title) in enumerate(zip(results, titles)):\n",
    "        axes[i].imshow(img)\n",
    "        axes[i].set_title(title, fontweight='bold')\n",
    "        axes[i].axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    print(\"Augmentation pipeline ran without errors.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Augmentation test failed: {e}\")\n",
    "\n",
    "# ROI training summary\n",
    "print(\"\\nROI-focused pipeline summary\")\n",
    "print(\"=\" * 28)\n",
    "print(\"Phase 1: ROI-supervised classification — patches from XML annotations\")\n",
    "print(\"Phase 2: MIL fine-tuning — initialized from Phase 1\")\n",
    "print(\"Phase 3: Segmentation — ROI regions converted to masks\")\n",
    "print(\"Augmentation pipeline — native H&E and elastic deformation\")\n",
    "print(\"Interpretability — Grad-CAM/attention overlays on selected samples\")\n",
    "\n",
    "# ROI annotation coverage\n",
    "metadata_file = legacy_config.DATA_DIR / \"metadata.csv\"\n",
    "if metadata_file.exists():\n",
    "    df = pd.read_csv(metadata_file)\n",
    "    total_slides = len(df)\n",
    "    roi_slides = len(df[df['has_annotation'] == True])\n",
    "    coverage = (roi_slides / total_slides) * 100 if total_slides > 0 else 0\n",
    "    print(\"\\nROI annotation coverage:\")\n",
    "    print(f\"  Total slides: {total_slides}\")\n",
    "    print(f\"  Slides with ROI annotations: {roi_slides}\")\n",
    "    print(f\"  Coverage: {coverage:.1f}%\")\n",
    "\n",
    "# Next steps\n",
    "print(\"\\nNext steps:\")\n",
    "print(\"1. Review wandb runs for metrics and artifacts.\")\n",
    "print(\"2. Run inference on held-out test data.\")\n",
    "print(\"3. Generate Grad-CAM or attention visualizations for qualitative review.\")\n",
    "print(\"4. Validate performance on ROI-annotated regions.\")\n",
    "print(\"5. Compare ROI-focused vs. random patch sampling if applicable.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75008eef",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Experiment completion and resource management\n",
    "============================================\n",
    "\n",
    "Final cleanup and structured summary of the experimental pipeline. Ensures proper resource management for reproducible runs.\n",
    "\"\"\"\n",
    "\n",
    "print(\"ROI-supervised HER2 classification pipeline complete\")\n",
    "print(\"=\" * 48)\n",
    "\n",
    "# Perform systematic cleanup\n",
    "print(\"Performing resource cleanup…\")\n",
    "\n",
    "try:\n",
    "    # Memory optimization\n",
    "    cleanup_variables = ['fig', 'axes', 'sample_image', 'results', 'titles', 'df', 'roi_slides']\n",
    "    cleaned_count = 0\n",
    "    \n",
    "    for var_name in cleanup_variables:\n",
    "        if var_name in locals():\n",
    "            del locals()[var_name]\n",
    "            cleaned_count += 1\n",
    "        if var_name in globals():\n",
    "            del globals()[var_name]\n",
    "            cleaned_count += 1\n",
    "    \n",
    "    # Force garbage collection\n",
    "    import gc\n",
    "    gc.collect()\n",
    "    \n",
    "    # GPU memory management\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "        gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
    "        print(f\"GPU memory cache cleared ({gpu_memory:.1f} GB total)\")\n",
    "    \n",
    "    print(f\"Resource cleanup complete ({cleaned_count} variables cleared)\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Cleanup note: {e}\")\n",
    "\n",
    "# Pipeline status summary\n",
    "print(\"\\nPipeline execution summary\")\n",
    "print(\"  ROI-supervised training framework initialized\")\n",
    "print(\"  Native H&E stain normalization pipeline available\")\n",
    "print(\"  Multi-phase training architecture configured\")\n",
    "print(\"  Experiment tracking enabled (wandb)\")\n",
    "print(\"  Model interpretability integrated (Grad-CAM)\")\n",
    "print(\"  Analysis tools prepared for publication\")\n",
    "\n",
    "# Output directory summary\n",
    "try:\n",
    "    from pathlib import Path\n",
    "    checkpoints_dir = Path(\"checkpoints\")\n",
    "    output_dir = Path(\"output\")\n",
    "    \n",
    "    print(\"\\nOutput locations:\")\n",
    "    print(f\"  Model checkpoints: {checkpoints_dir}\")\n",
    "    print(f\"  Training logs: {output_dir / 'logs'}\")\n",
    "    print(f\"  Visualizations: {output_dir / 'logs' / 'explanations'}\")\n",
    "    print(f\"  Experiment data: Weights & Biases dashboard\")\n",
    "    \n",
    "except Exception:\n",
    "    print(\"\\nStandard output directories: checkpoints/, output/\")\n",
    "\n",
    "# Research and publication guidelines\n",
    "print(\"\\nResearch guidelines:\")\n",
    "print(\"  • Ensure proper data licensing and ethical approval\")\n",
    "print(\"  • Validate results with independent test datasets\")\n",
    "print(\"  • Report confidence intervals and statistical significance\")\n",
    "print(\"  • Document hardware specifications and runtime requirements\")\n",
    "print(\"  • Preserve experiment artifacts for reproducibility\")\n",
    "\n",
    "print(\"\\nClinical validation recommendations:\")\n",
    "print(\"  • Collaborate with board-certified pathologists\")\n",
    "print(\"  • Perform inter-observer agreement studies\")\n",
    "print(\"  • Validate across multiple institutions\")\n",
    "print(\"  • Consider prospective studies when appropriate\")\n",
    "\n",
    "print(\"\\nSystem prepared for peer review and publication submission\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
